{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import MDP\n",
    "\n",
    "class RL:\n",
    "    def __init__(self,mdp,sampleReward):\n",
    "        '''Constructor for the RL class\n",
    "\n",
    "        Inputs:\n",
    "        mdp -- Markov decision process (T, R, discount)\n",
    "        sampleReward -- Function to sample rewards (e.g., bernoulli, Gaussian).\n",
    "        This function takes one argument: the mean of the distributon and \n",
    "        returns a sample from the distribution.\n",
    "        '''\n",
    "\n",
    "        self.mdp = mdp\n",
    "        self.sampleReward = sampleReward\n",
    "\n",
    "    def sampleRewardAndNextState(self,state,action):\n",
    "        '''Procedure to sample a reward and the next state\n",
    "        reward ~ Pr(r)\n",
    "        nextState ~ Pr(s'|s,a)\n",
    "\n",
    "        Inputs:\n",
    "        state -- current state\n",
    "        action -- action to be executed\n",
    "\n",
    "        Outputs: \n",
    "        reward -- sampled reward\n",
    "        nextState -- sampled next state\n",
    "        '''\n",
    "\n",
    "        reward = self.sampleReward(self.mdp.R[action,state])\n",
    "        cumProb = np.cumsum(self.mdp.T[action,state,:])\n",
    "        nextState = np.where(cumProb >= np.random.rand(1))[0][0]\n",
    "        return [reward,nextState]\n",
    "\n",
    "    def qLearning(self,s0,initialQ,nEpisodes,nSteps,epsilon=0,temperature=0):\n",
    "        '''qLearning algorithm.  Epsilon exploration and Boltzmann exploration\n",
    "        are combined in one procedure by sampling a random action with \n",
    "        probabilty epsilon and performing Boltzmann exploration otherwise.  \n",
    "        When epsilon and temperature are set to 0, there is no exploration.\n",
    "\n",
    "        Inputs:\n",
    "        s0 -- initial state\n",
    "        initialQ -- initial Q function (|A|x|S| array)\n",
    "        nEpisodes -- # of episodes (one episode consists of a trajectory of nSteps that starts in s0\n",
    "        nSteps -- # of steps per episode\n",
    "        epsilon -- probability with which an action is chosen at random\n",
    "        temperature -- parameter that regulates Boltzmann exploration\n",
    "\n",
    "        Outputs: \n",
    "        Q -- final Q function (|A|x|S| array)\n",
    "        policy -- final policy\n",
    "        '''\n",
    "\n",
    "        Q = np.zeros([self.mdp.nActions,self.mdp.nStates])\n",
    "        policy = np.zeros(self.mdp.nStates,int)\n",
    "        temperature = float(temperature)\n",
    "        counts = np.zeros([self.mdp.nActions, self.mdp.nStates])\n",
    "\n",
    "\n",
    "        for episode in range(nEpisodes):\n",
    "            s = s0\n",
    "\n",
    "            for step in range(nSteps):\n",
    "                # Action selection\n",
    "                if np.random.random() < epsilon:\n",
    "                    a = np.random.randint(self.mdp.nActions)\n",
    "                else:\n",
    "                    if temperature == 0.:\n",
    "                        a = np.argmax(Q[:, s])\n",
    "                    else:\n",
    "                        prob_a = np.exp(Q[:, s] / temperature) / np.sum(np.exp(Q[:, s] / temperature))\n",
    "                        a = np.argmax(np.random.multinomial(1, prob_a))\n",
    "\n",
    "                # Observe s' and r\n",
    "                r, s_prime = self.sampleRewardAndNextState(s, a)\n",
    "\n",
    "                # Update count\n",
    "                counts[a, s] += 1.\n",
    "\n",
    "                # Learning rate\n",
    "                alpha = counts[a,s]\n",
    "\n",
    "                # Update Q\n",
    "                Q[a,s] += alpha*(r + self.mdp.discount*np.amax(Q[:,s_prime], axis=0) - Q[a,s])\n",
    "\n",
    "                s = s_prime\n",
    "\n",
    "                if np.linalg.norm(prev_Q - Q[a,s]) <= 1e-8:\n",
    "                    break\n",
    "                \n",
    "        policy = np.argmax(Q, axis=0)\n",
    "\n",
    "        return [Q,policy]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Construct simple MDP as described in Lecture 2a Slides 13-14'''\n",
    "T = np.array([[[0.5,0.5,0,0],[0,1,0,0],[0.5,0.5,0,0],[0,1,0,0]],[[1,0,0,0],[0.5,0,0,0.5],[0.5,0,0.5,0],[0,0,0.5,0.5]]])\n",
    "R = np.array([[0,0,10,10],[0,0,10,10]])\n",
    "discount = 0.9        \n",
    "mdp = MDP.MDP(T,R,discount)\n",
    "rlProblem = RL(mdp,np.random.normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_1980\\4271257844.py:86: RuntimeWarning: overflow encountered in double_scalars\n",
      "  Q[a,s] += alpha*(r + self.mdp.discount*np.amax(Q[:,s_prime], axis=0) - Q[a,s])\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_1980\\4271257844.py:86: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  Q[a,s] += alpha*(r + self.mdp.discount*np.amax(Q[:,s_prime], axis=0) - Q[a,s])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-learning results\n",
      "[[nan nan nan nan]\n",
      " [nan nan nan nan]]\n",
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Test Q-learning \n",
    "[Q,policy] = rlProblem.qLearning(s0=0,initialQ=np.zeros([mdp.nActions,mdp.nStates]),nEpisodes=1000,nSteps=100,epsilon=0.3)\n",
    "print(\"\\nQ-learning results\")\n",
    "print(Q)\n",
    "print(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
