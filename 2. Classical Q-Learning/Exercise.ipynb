{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a938939",
   "metadata": {},
   "source": [
    "## <center> Exercise with continuos Q-Learning</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147ae69",
   "metadata": {},
   "source": [
    "In this exercise we take a look at the MountainCar-v0 (https://gym.openai.com/envs/MountainCar-v0/), which has the goal to reach the top of the mountain within some time limit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f7e28",
   "metadata": {},
   "source": [
    "**TASK: Create the gym mountain car environment** <br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall():\n",
    "    \"\"\"\n",
    "    Each time a reset or a render are called, the environment has to be recharged or recalled.\n",
    "    In the recall function the name and the make of the environment must me set. \n",
    "    \"\"\"\n",
    "    env_name = \"MountainCar-v0\"  # Use the exact same name as stated on gym.openai\n",
    "    env = gym.make(env_name)  # use gym.make to create your environment, important declare the render_mode\n",
    "\n",
    "    return env\n",
    "\n",
    "env = recall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171192c3",
   "metadata": {},
   "source": [
    "**TASK: Write a code to create a numpy array holding the bins for the observations of the car (position and velocity).**\n",
    "\n",
    "The function should take one argument which acts as the bins per observation Hint: You will probably need around 25 bins for good results, but feel free to use less to reduce training time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BINS = 50\n",
    "\n",
    "position_bin = np.linspace(-1.2, 0.6, NUM_BINS)\n",
    "velocity_bin = np.linspace(-0.07, 0.07, NUM_BINS)\n",
    "\n",
    "BINS = [position_bin, velocity_bin]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28446881",
   "metadata": {},
   "source": [
    "**TASK: Create a function that will take in observations from the environment and the bins array and return the discretized version of the observation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binner(observations, bins):\n",
    "    binned_observations = []\n",
    "\n",
    "    for ind, observation in enumerate(observations):\n",
    "        binned_val = np.digitize(observation, bins[ind])\n",
    "        binned_observations.append(binned_val)\n",
    "    \n",
    "    return tuple(binned_observations) # Important for later indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d8d8e",
   "metadata": {},
   "source": [
    "**TASK: Confirm that your *binner()* function works by running the following cell***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eca009",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin1 =  [-1.2 , -0.75, -0.3 ,  0.15,  0.6]\n",
    "bin2 = [-0.07 , -0.035,  0.   ,  0.035,  0.07 ]\n",
    "test_bins = [bin1, bin2]\n",
    "\n",
    "test_observation = np.array([-0.9, 0.03])\n",
    "discretized_test_bins = binner(test_observation, test_bins)\n",
    "assert discretized_test_bins == (1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e71a8",
   "metadata": {},
   "source": [
    "**TASK: Create the Q-Table** <br />\n",
    "Remember the shape that the Q-Table needs to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b9bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table_shape = (NUM_BINS, NUM_BINS, env.action_space.n)\n",
    "q_table = np.zeros(q_table_shape)\n",
    "print(q_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d485ec",
   "metadata": {},
   "source": [
    "**TASK: Fill out the Epislon Greedy Action Selection function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac943d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_selection(epsilon, q_table, discrete_state):\n",
    "    random_number = np.random.random()\n",
    "    \n",
    "    # EXPLOITATION, USE BEST Q(s,a) Value\n",
    "    if random_number > epsilon:\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "\n",
    "    # EXPLORATION, USE A RANDOM ACTION\n",
    "    else:\n",
    "        action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f5ec4",
   "metadata": {},
   "source": [
    "**TASK: Fill out the function to compute the next Q value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_q_value(old_q_value, reward, next_optimal_q_value):\n",
    "    \n",
    "    return old_q_value +  ALPHA * (reward + GAMMA * next_optimal_q_value - old_q_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043548dc",
   "metadata": {},
   "source": [
    "**TASK: Create a function to reduce epsilon, feel free to choose any reduction method you want. We'll use a reduction with BURN_IN and EPSILON_END limits in the solution. We'll also show a way to reduce epsilon based on the number of epochs. Feel free to experiment here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea236a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_epsilon(epsilon, epoch):\n",
    "    if BURN_IN <= epoch <= EPSILON_END:\n",
    "        epsilon -= EPSILON_REDUCE\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2631b38",
   "metadata": {},
   "source": [
    "**TASK: Define your hyperparameters. Note, we'll show our solution hyperparameters here, but depending on your *reduce_epsilon* function, your epsilon hyperparameters may be different.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60825d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30000\n",
    "BURN_IN = 100\n",
    "epsilon = 1\n",
    "\n",
    "EPSILON_END= 10000\n",
    "EPSILON_REDUCE = 0.0001 \n",
    "\n",
    "ALPHA = 0.8\n",
    "GAMMA = 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73abcf6",
   "metadata": {},
   "source": [
    "**TASK: Create the training loop for the reinforcement learning agent and run the loop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists\n",
    "\n",
    "points = []\n",
    "mean_points = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # Reset the environment\n",
    "    env = recall()\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    binned_state = binner(state, BINS)\n",
    "    \n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        action = action_selection(epsilon, q_table, binned_state)\n",
    "\n",
    "        next_state, reward, done, *info = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "        old_q_value =  q_table[binned_state + (action,)]\n",
    "\n",
    "        binned_next_state = binner(next_state, BINS) \n",
    "        next_optimal_q_value = np.max(q_table[binned_next_state])  \n",
    "\n",
    "        next_q = next_q_value(old_q_value, reward, next_optimal_q_value)   \n",
    "\n",
    "        q_table[binned_state + (action,)] = next_q\n",
    "        \n",
    "        binned_state = binned_next_state\n",
    "\n",
    "    epsilon = reduce_epsilon(epsilon,epoch)\n",
    "\n",
    "    points.append(score)\n",
    "    running_mean = round(np.mean(points[-50:]), 2)\n",
    "    mean_points.append(running_mean) \n",
    "\n",
    "    print(epoch)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de93aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(np.arange(0, EPOCHS, 1), points)\n",
    "ax.plot(np.arange(0, EPOCHS, 1), points)\n",
    "ax.plot(np.arange(0, EPOCHS, 1), mean_points, label=f\"Running Mean: {running_mean}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5de67a",
   "metadata": {},
   "source": [
    "**TASK: Use your Q-Table to test your agent and render its performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbe789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\") \n",
    "observation = env.reset()\n",
    "observation = observation[0]\n",
    "rewards = 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    discrete_state = binner(observation, BINS)  # get bins\n",
    "    action = np.argmax(q_table[discrete_state])  # and chose action from the Q-Table\n",
    "    observation, reward, done, *info = env.step(action) # Finally perform the action\n",
    "    rewards += reward\n",
    "    if done:\n",
    "        print(f\"You got {rewards} points!\")\n",
    "        break\n",
    "\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
