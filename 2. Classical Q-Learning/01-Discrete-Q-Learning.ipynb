{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "green-recommendation",
   "metadata": {},
   "source": [
    "## <center> Q-Learning - Discrete Actions</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "billion-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b461afe9",
   "metadata": {},
   "source": [
    "### PART 1: \n",
    "\n",
    "Setting up Frozen Lake Environment\n",
    "\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "    S | F | F | F\n",
    "    - - - - - - -\n",
    "    F | H | F | H\n",
    "    - - - - - - -\n",
    "    F | F | F | H\n",
    "    - - - - - - -\n",
    "    H | F | F | G  \n",
    "\n",
    "- S: starting point, safe \n",
    "- F: frozen surface, safe)\n",
    "- H: hole, fall to your doom)\n",
    "- G: goal, where the frisbee is located\n",
    "\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise. Link to Environment: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "We will be removing \"slippery\" from the environment, so there is no randomness to the direction. More info: https://github.com/openai/gym/issues/565"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b575ac9",
   "metadata": {},
   "source": [
    "----\n",
    "And finally, depending on your computer specs or patience, you can scale the size of the lake up or down:\n",
    "\n",
    "https://stackoverflow.com/questions/55006689/how-to-generate-a-random-frozen-lake-map-in-openai\n",
    "\n",
    "To keep things simply, we'll use the default 4by4 grid, but check out the link above if you're crazy enough to go to some huge N by N grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7227211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "# random_map = generate_random_map(size=3, p=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a433fe",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e55db",
   "metadata": {},
   "source": [
    "#### Environment set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69704fa",
   "metadata": {},
   "source": [
    "**Here we register a new environment, where is_slippery=False.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdd8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "try:\n",
    "\n",
    "    register(\n",
    "        id='FrozenLakeNotSlippery-v0', # make sure this is a custom name!\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "        max_episode_steps=100, # if goal not completed stops after this number of episodes\n",
    "        reward_threshold=.8196, # optimum = .8196 --> not useful for binary goals\n",
    "    )\n",
    "except:\n",
    "    print('You probably ran this cell twice, accidentally trying to register a new env with the same id twice.')\n",
    "    print(\"Either change the id, or just continue, knowing your id was already registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0e9f80",
   "metadata": {},
   "source": [
    "**Let's run it with some random actions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "southern-science",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")  # Load FrozenLake\n",
    "env.reset()  # Reset to initial state\n",
    "for _ in range(5):\n",
    "    a = env.render(mode=\"ansi\")  # Render on the screen\n",
    "    print(a)\n",
    "    action = env.action_space.sample()  # chose a random action\n",
    "    env.step(action)  # Perform random action on the environment\n",
    "    time.sleep(0.5)\n",
    "env.close()  # dont forget to close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f3e94c",
   "metadata": {},
   "source": [
    "#### Cleaning the output through each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42725f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")  # Load FrozenLake\n",
    "env.reset()\n",
    "\n",
    "for _ in range(5):\n",
    "    clear_output(wait=True) # Clears the previous output\n",
    "    a = env.render(mode=\"ansi\") \n",
    "    print(a)\n",
    "    action = env.action_space.sample()  \n",
    "    env.step(action) \n",
    "    time.sleep(0.5)\n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad42fc",
   "metadata": {},
   "source": [
    "#### Gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "746a6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asdw():\n",
    "    '''\n",
    "    This function gets the key press for gym action choice\n",
    "    '''\n",
    "    k = input()\n",
    "    if k == 'a':\n",
    "        action = 0\n",
    "    if k == 's':\n",
    "        action = 1\n",
    "    if k == 'd':\n",
    "        action = 2\n",
    "    if k == 'w':\n",
    "        action = 3\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1060c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Done\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")  # Load FrozenLake\n",
    "env.reset()  # Reset to initial state\n",
    "for _ in range(10):\n",
    "    env.render(mode=\"human\")  # Render on the screen\n",
    "    clear_output(wait=True)\n",
    "    action = asdw()  # chose an action\n",
    "    observation, reward, done, info = env.step(action)  # Perform random action on the environment\n",
    "    \n",
    "    if done:\n",
    "        print(\"Game Done\")\n",
    "        print(f\"Reward: {reward}\")\n",
    "        break\n",
    "env.close()  # dont forget to close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-glasgow",
   "metadata": {},
   "source": [
    "### PART 2: \n",
    "\n",
    "Creating the Q-Learning Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-contest",
   "metadata": {},
   "source": [
    "Now that we validated the functionality of our function it is time to move on with the Q-Learning algorithm. \n",
    "\n",
    "Recall our Table is essentially a mapping of all possible state, action pairs and the expected reward for taking an action at a particular state that we will keep updating.\n",
    "\n",
    "\n",
    "$$Q(s_t,a_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542e0f83",
   "metadata": {},
   "source": [
    "For our simple discrete Frozen Lake problem, this means we have 4 actions for columns, and 16 possible states (player location on the 4 by 4 grid). So our table will look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa8c3e",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <th></th>\n",
    "    <th>A0 - LEFT</th>\n",
    "    <th>A1 - DOWN</th>\n",
    "    <th>A2 - RIGHT</th>\n",
    "    <th>A3 - UP</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>State 0</strong></td>\n",
    "    <td>Q(s,a)</td>\n",
    "    <td>Q(s,a)</td>\n",
    "      <td>Q(s,a)</td>\n",
    "      <td>Q(s,a)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td><strong>State 1</strong></td>\n",
    "    <td>Q(s,a)</td>\n",
    "    <td>Q(s,a)</td>\n",
    "    <td>Q(s,a)</td>\n",
    "      <td>Q(s,a)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "      <td><strong>State ...</strong></td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "        <td>...</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "      <td><strong>State 15</strong></td>\n",
    "    <td>Q(s,a)</td>\n",
    "    <td>Q(s,a)</td>\n",
    "    <td>Q(s,a)</td>\n",
    "        <td>Q(s,a)</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec3a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14e800e",
   "metadata": {},
   "source": [
    "#### Initial Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "latter-header",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with very small values for all our Q(s,a)\n",
    "q_table = np.zeros([state_size, action_size])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45fe3323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9f3fa",
   "metadata": {},
   "source": [
    "### PART 3:\n",
    "\n",
    "Hyperparameters\n",
    "\n",
    "The Q-Learning update functions will require hyperparameters. we'll define them here. Often the best place to choose a good starting value is reading publications or through experimentation. Unfortunately, its very difficult to give general advice, as most environments are radically different to each other, and often hyperparameter tuning is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8666bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is common to leave Hyperparameters in ALL CAPS to easily locate them\n",
    "\n",
    "EPOCHS = 20000  # number of epochs/episodes to train for\n",
    "ALPHA = 0.8 # aka the learning rate\n",
    "GAMMA = 0.95 # aka the discount rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fdd6e6",
   "metadata": {},
   "source": [
    "**Exploration vs. Exploitation Parameters**\n",
    "\n",
    "Basically how fast do we reduce epsilon. Reduce too fast, agent won't have enough time to learn. Reduce too slow, you're wasting time picking random actions. Key here is that these value help balance exploration (random choice) versus explotation (always picking what works for that Q(s,a). It's a tough balance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd70a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration vs. Exploitation parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.001            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d29c33",
   "metadata": {},
   "source": [
    "### PART 4:\n",
    "Q-Table Update Functions Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-saturday",
   "metadata": {},
   "source": [
    "Now it is time to dive into the training / Q-Table update methodology. First we will define some functions needed for training phase:\n",
    "\n",
    "1. Action selection:\n",
    "\n",
    "* epsilon_greedy_action_selection: Is used to implement the epsilon greedy action selection routine.\n",
    "* compute_next_q_value: Computes the next Q-Values according to the formula from the lecture\n",
    "* reduce_epsilon: Reduces the $\\epsilon$ used for the epsilon greedy algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79070b6",
   "metadata": {},
   "source": [
    "**1. FUNCTION TO SELECT AN ACTION**\n",
    "\n",
    "If we simply always select the argmax() Q-table value during training, we'll most likely get stuck in an explotation loop, so we'll use a random value to randomly select an action from time to time, helping the model explore, rather than exploit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38439bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_selection(epsilon, q_table, discrete_state):\n",
    "    \"\"\"\n",
    "    Returns an action for the agent. Note how it uses a random number to decide on exploration \n",
    "    versus explotation trade-off.\n",
    "    \"\"\"\n",
    "    \n",
    "    random_number = np.random.random()\n",
    "    \n",
    "    # EXPLOITATION, USE BEST Q(s,a) Value\n",
    "    if random_number > epsilon:\n",
    "        # Action row for a particular state\n",
    "        state_row = q_table[discrete_state,:]\n",
    "\n",
    "        # Index of highest action for state\n",
    "        action = np.argmax(state_row, axis=0)\n",
    "            \n",
    "    # EXPLORATION, USE A RANDOM ACTION\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a4e89",
   "metadata": {},
   "source": [
    "**2. FUNCTION FOR Q_VALUE COMPUTATION**\n",
    "\n",
    "\n",
    "$$Q(s,a) \\gets (1-\\alpha)*Q(s,a) + \\alpha*[R(s,a) + \\gamma*\\max_{a}Q(s_{t+1}, a)]$$\n",
    "\n",
    "Here we have our main Q-Learning update equation, note how it takes in the old q-value, the next optimal q value, along with our current reward, and then updates the next q value accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17ec3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_update(old_q_value, reward, next_optimal_q_value):\n",
    "    new_q = (1-ALPHA)*old_q_value +  ALPHA * (reward + GAMMA * next_optimal_q_value)\n",
    "    return new_q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e05888",
   "metadata": {},
   "source": [
    "**3. FUNCTION TO REDUCE EPSILON**\n",
    "\n",
    "As training continues, we need to balance explotation versus exploration, we want ot make sure our agent doesn't get trapped in a cycle going from an square to another square back and forth. We also don't want our agent permanently choosing random values. We'll use the function below to try to balance this.\n",
    "\n",
    "$$ \\epsilon = \\epsilon_{min} + (\\epsilon_{max} - \\epsilon_{min})*e^{-\\lambda*\\tau}$$\n",
    "\n",
    "- $\\lambda$: decay rate\n",
    "- $\\tau$: epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74c791f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_epsilon(epsilon,epoch):\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*epoch)\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c42807",
   "metadata": {},
   "source": [
    "### PART 5:\n",
    "\n",
    "Training of Agent and Updating Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d86c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset just in case\n",
    "q_table = np.zeros([state_size, action_size])\n",
    "total_reward = 0\n",
    "epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "080df6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# Play 20k games\n",
    "for episode in range(EPOCHS):\n",
    "\n",
    "    # Reset the environment\n",
    "    env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "    # To visualize the whole traninig, change render_mode to \"human\" --> takes a lot of time\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = action_selection(epsilon, q_table, state)\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        \n",
    "        # Look up current/old qtable value Q(s_t,a_t)\n",
    "        old_q_value =  q_table[state,action]  \n",
    "\n",
    "        # Get the next optimal Q-Value\n",
    "        next_optimal_q_value = np.max(q_table[new_state, :])  \n",
    "\n",
    "        # Update q value\n",
    "        q_table[state,action] = q_update(old_q_value, reward, next_optimal_q_value)\n",
    "\n",
    "\n",
    "        total_rewards = total_rewards + reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "\n",
    "        \n",
    "    episode += 1\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = reduce_epsilon(epsilon,episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08384087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d77cc93e80>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(EPOCHS),np.cumsum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab364621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
       "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
       "       [0.81450625, 0.        , 0.77378017, 0.77378093],\n",
       "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
       "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
       "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
       "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33b897",
   "metadata": {},
   "source": [
    "### PART 5:\n",
    "Using Learned Q Table Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-adventure",
   "metadata": {},
   "source": [
    "Now it is time for a final evaluation round!\n",
    "Let's see how well our first RL agent performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28938392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    a = env.render(mode=\"ansi\") \n",
    "    print(a)\n",
    "    \n",
    "    action = np.argmax(q_table[state])  # and chose action from the Q-Table\n",
    "    state, reward, done, info = env.step(action) # Finally perform the action\n",
    "\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
