{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bd-uUpa1gcss"
      },
      "source": [
        "## <center> Costum environment creation with OpenAI GYM </center>\n",
        "\n",
        "In the following notebook a costum environment will be created, in which an agent will have to adjust the temperature of a shower. During this, the agenet will have to deal with temperature fluctuations and a certain time limit."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kBUC8IZzgcsx"
      },
      "source": [
        "### 1. Creating the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dYtGONFtgcsy"
      },
      "outputs": [],
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "evjdhHX5gcs0"
      },
      "outputs": [],
      "source": [
        "class ShowerEnv(Env):\n",
        "    def __init__(self):\n",
        "        # Actions we can take, down, stay, up\n",
        "        self.action_space = Discrete(3)\n",
        "        # Temperature array\n",
        "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
        "        # Set start temp and start time\n",
        "        self.reset()\n",
        "        \n",
        "    def step(self, action):\n",
        "\n",
        "        # Apply action\n",
        "        self.state += action - 1\n",
        "        # Decrease temperature action = 0 --> T + 0 - 1 = T - 1\n",
        "        # Stall temperature action = 1 --> T + 1 - 1 = T + 0\n",
        "        # Increase temperature action = 2 --> T + 2 -1 = T + 1\n",
        "        \n",
        "        \n",
        "        # Apply temperature noise --> more realistic model\n",
        "        self.state += random.randint(-1,1)\n",
        "\n",
        "        # Reduce shower length by 1 second\n",
        "        self.shower_length -= 1 \n",
        "        \n",
        "        # Calculate reward\n",
        "        if 37 <= self.state <= 39: \n",
        "            reward = 1 \n",
        "        else: \n",
        "            reward = -1 \n",
        "        \n",
        "        # Check if shower is done\n",
        "        if self.shower_length <= 0: \n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "        \n",
        "        # Set placeholder for info\n",
        "        info = {}\n",
        "        \n",
        "        # Return step information\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def render(self):\n",
        "        # Implement visualization --> in this case is not built\n",
        "        pass\n",
        "    \n",
        "    def reset(self):\n",
        "        # Reset shower temperature\n",
        "        self.state = 38 + random.randint(-3,3)\n",
        "        # Reset shower time\n",
        "        self.shower_length = 60 \n",
        "        return self.state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al5Psst7gcs1",
        "outputId": "163df462-a328-4386-a653-f9ae5f48e04c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 1, Mean temperature: 39.20 Score: -44\n",
            "Episode: 2, Mean temperature: 38.73 Score: -30\n",
            "Episode: 3, Mean temperature: 33.67 Score: -34\n",
            "Episode: 4, Mean temperature: 28.75 Score: -60\n",
            "Episode: 5, Mean temperature: 44.00 Score: -48\n",
            "Episode: 6, Mean temperature: 40.83 Score: -48\n",
            "Episode: 7, Mean temperature: 40.28 Score: -26\n",
            "Episode: 8, Mean temperature: 47.65 Score: -50\n",
            "Episode: 9, Mean temperature: 39.90 Score: -28\n",
            "Episode: 10, Mean temperature: 38.80 Score: -46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        }
      ],
      "source": [
        "env = ShowerEnv()\n",
        "episodes = 10\n",
        "\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    temps = []\n",
        "    \n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        temp, reward, done, info = env.step(action)\n",
        "        temps.append(temp) \n",
        "        score +=reward\n",
        "    mean_temp = np.mean(np.array(temps))\n",
        "    print(f'Episode: {episode}, Mean temperature: {mean_temp:.2f} Score: {score}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kJMVw1x5gcs2"
      },
      "source": [
        "### 2. Creating a Deep Learning Model Agent with Keras-RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z1R8N6ffgcs2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k51cTySQgcs3",
        "outputId": "5f6ff1c2-fb9b-4405-d586-bbfe578e5988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 3 possible actions and 1 observations\n"
          ]
        }
      ],
      "source": [
        "num_actions = env.action_space.n\n",
        "num_observations = env.observation_space.shape[0]\n",
        "print(f\"There are {num_actions} possible actions and {num_observations} observations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0R7bQzObgcs4"
      },
      "outputs": [],
      "source": [
        "def build_model(states, actions):\n",
        "    model = Sequential()    \n",
        "    model.add(Dense(128, activation='relu', input_shape=(states, )))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(18, activation='relu'))\n",
        "    model.add(Dense(9, activation='relu'))\n",
        "\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZw8M9vDgcs5",
        "outputId": "bde9f235-e453-4300-e7db-f62d8625c2c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               256       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 18)                594       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 9)                 171       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3)                 30        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,387\n",
            "Trainable params: 11,387\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = build_model(num_observations, num_actions)\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KSzGvTQhgcs5"
      },
      "outputs": [],
      "source": [
        "memory = SequentialMemory(limit=20000, window_length=1)\n",
        "policy = LinearAnnealedPolicy(BoltzmannQPolicy(),\n",
        "                              attr='tau', \n",
        "                              value_max=1.0, \n",
        "                              value_min=0.001,  \n",
        "                              value_test=0.0005, \n",
        "                              nb_steps=400000) \n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=num_actions, \n",
        "               memory=memory, \n",
        "               nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy)\n",
        "\n",
        "# Compilation\n",
        "dqn.compile(Adam(learning_rate=0.0001), metrics=['mae']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpK2bhGsgcs6",
        "outputId": "cd887f51-0f6f-4355-b5b7-e43bde9f76fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 400000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "\r    1/10000 [..............................] - ETA: 22:31 - reward: -1.0000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   20/10000 [..............................] - ETA: 15:20 - reward: -1.0000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   40/10000 [..............................] - ETA: 8:25 - reward: -0.6500"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 95s 9ms/step - reward: -0.6916\n",
            "166 episodes - episode_reward: -41.518 [-60.000, 12.000] - loss: 0.983 - mae: 6.615 - mean_q: -9.267 - mean_tau: 0.988\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 93s 9ms/step - reward: -0.7450\n",
            "167 episodes - episode_reward: -44.754 [-60.000, 12.000] - loss: 1.475 - mae: 9.593 - mean_q: -13.622 - mean_tau: 0.963\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: -0.7272\n",
            "167 episodes - episode_reward: -43.545 [-60.000, 12.000] - loss: 1.585 - mae: 10.449 - mean_q: -14.902 - mean_tau: 0.938\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 93s 9ms/step - reward: -0.6760\n",
            "166 episodes - episode_reward: -40.687 [-60.000, 18.000] - loss: 2.275 - mae: 12.110 - mean_q: -17.435 - mean_tau: 0.913\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: -0.5652\n",
            "167 episodes - episode_reward: -33.784 [-60.000, 22.000] - loss: 3.342 - mae: 14.028 - mean_q: -20.328 - mean_tau: 0.888\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: -0.4956\n",
            "167 episodes - episode_reward: -29.772 [-60.000, 28.000] - loss: 2.747 - mae: 12.260 - mean_q: -17.637 - mean_tau: 0.863\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 93s 9ms/step - reward: -0.3670\n",
            "166 episodes - episode_reward: -22.060 [-60.000, 24.000] - loss: 1.776 - mae: 9.263 - mean_q: -13.118 - mean_tau: 0.838\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: -0.2712\n",
            "167 episodes - episode_reward: -16.323 [-60.000, 30.000] - loss: 1.123 - mae: 6.479 - mean_q: -8.918 - mean_tau: 0.813\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: 0.1154\n",
            "167 episodes - episode_reward: 6.946 [-42.000, 46.000] - loss: 0.620 - mae: 2.184 - mean_q: -2.274 - mean_tau: 0.788\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 96s 10ms/step - reward: 0.1350\n",
            "166 episodes - episode_reward: 7.976 [-60.000, 44.000] - loss: 0.783 - mae: 1.904 - mean_q: 1.989 - mean_tau: 0.763\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 97s 10ms/step - reward: 0.3874\n",
            "167 episodes - episode_reward: 23.365 [-20.000, 46.000] - loss: 3.891 - mae: 12.825 - mean_q: 19.252 - mean_tau: 0.738\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: 0.4092\n",
            "167 episodes - episode_reward: 24.491 [-4.000, 48.000] - loss: 6.577 - mae: 17.934 - mean_q: 26.900 - mean_tau: 0.713\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 93s 9ms/step - reward: 0.4788\n",
            "166 episodes - episode_reward: 28.699 [2.000, 52.000] - loss: 7.643 - mae: 19.461 - mean_q: 29.305 - mean_tau: 0.688\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 93s 9ms/step - reward: 0.4756\n",
            "167 episodes - episode_reward: 28.551 [4.000, 52.000] - loss: 8.445 - mae: 20.401 - mean_q: 30.733 - mean_tau: 0.663\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 92s 9ms/step - reward: 0.5056\n",
            "167 episodes - episode_reward: 30.347 [0.000, 58.000] - loss: 8.612 - mae: 21.096 - mean_q: 31.788 - mean_tau: 0.638\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 92s 9ms/step - reward: 0.5300\n",
            "166 episodes - episode_reward: 31.795 [6.000, 54.000] - loss: 9.259 - mae: 21.785 - mean_q: 32.874 - mean_tau: 0.613\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 92s 9ms/step - reward: 0.5658\n",
            "167 episodes - episode_reward: 33.904 [6.000, 54.000] - loss: 9.702 - mae: 22.337 - mean_q: 33.709 - mean_tau: 0.588\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 95s 9ms/step - reward: 0.5924\n",
            "167 episodes - episode_reward: 35.581 [4.000, 56.000] - loss: 9.843 - mae: 22.553 - mean_q: 34.020 - mean_tau: 0.563\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: 0.5360\n",
            "166 episodes - episode_reward: 32.181 [8.000, 52.000] - loss: 9.528 - mae: 22.388 - mean_q: 33.724 - mean_tau: 0.538\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 93s 9ms/step - reward: 0.5626\n",
            "167 episodes - episode_reward: 33.737 [0.000, 52.000] - loss: 9.418 - mae: 21.866 - mean_q: 32.962 - mean_tau: 0.513\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 92s 9ms/step - reward: 0.6294\n",
            "167 episodes - episode_reward: 37.749 [4.000, 58.000] - loss: 9.244 - mae: 21.612 - mean_q: 32.665 - mean_tau: 0.488\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: 0.5890\n",
            "166 episodes - episode_reward: 35.277 [6.000, 54.000] - loss: 9.464 - mae: 21.795 - mean_q: 32.831 - mean_tau: 0.463\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: 0.6312\n",
            "167 episodes - episode_reward: 37.964 [12.000, 58.000] - loss: 9.540 - mae: 21.987 - mean_q: 33.177 - mean_tau: 0.438\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 95s 10ms/step - reward: 0.6412\n",
            "167 episodes - episode_reward: 38.431 [-2.000, 56.000] - loss: 9.858 - mae: 22.427 - mean_q: 33.837 - mean_tau: 0.413\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 93s 9ms/step - reward: 0.6450\n",
            "166 episodes - episode_reward: 38.759 [2.000, 58.000] - loss: 9.949 - mae: 22.431 - mean_q: 33.874 - mean_tau: 0.388\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 92s 9ms/step - reward: 0.6764\n",
            "167 episodes - episode_reward: 40.515 [16.000, 58.000] - loss: 9.656 - mae: 22.486 - mean_q: 33.931 - mean_tau: 0.363\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 92s 9ms/step - reward: 0.6668\n",
            "167 episodes - episode_reward: 40.012 [14.000, 58.000] - loss: 9.823 - mae: 22.673 - mean_q: 34.187 - mean_tau: 0.338\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 93s 9ms/step - reward: 0.6634\n",
            "166 episodes - episode_reward: 39.819 [-20.000, 56.000] - loss: 9.640 - mae: 22.443 - mean_q: 33.758 - mean_tau: 0.313\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: 0.7128\n",
            "167 episodes - episode_reward: 42.719 [12.000, 60.000] - loss: 10.016 - mae: 22.774 - mean_q: 34.396 - mean_tau: 0.288\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            "10000/10000 [==============================] - 93s 9ms/step - reward: 0.7756\n",
            "167 episodes - episode_reward: 46.551 [28.000, 60.000] - loss: 10.707 - mae: 23.430 - mean_q: 35.474 - mean_tau: 0.263\n",
            "\n",
            "Interval 31 (300000 steps performed)\n",
            "10000/10000 [==============================] - 93s 9ms/step - reward: 0.7404\n",
            "166 episodes - episode_reward: 44.458 [8.000, 60.000] - loss: 10.689 - mae: 23.943 - mean_q: 36.208 - mean_tau: 0.238\n",
            "\n",
            "Interval 32 (310000 steps performed)\n",
            "10000/10000 [==============================] - 95s 9ms/step - reward: 0.7252\n",
            "167 episodes - episode_reward: 43.473 [12.000, 58.000] - loss: 10.504 - mae: 23.294 - mean_q: 35.081 - mean_tau: 0.213\n",
            "\n",
            "Interval 33 (320000 steps performed)\n",
            "10000/10000 [==============================] - 102s 10ms/step - reward: 0.7786\n",
            "167 episodes - episode_reward: 46.719 [22.000, 60.000] - loss: 9.644 - mae: 22.755 - mean_q: 34.284 - mean_tau: 0.188\n",
            "\n",
            "Interval 34 (330000 steps performed)\n",
            "10000/10000 [==============================] - 95s 10ms/step - reward: 0.7472\n",
            "166 episodes - episode_reward: 44.831 [10.000, 60.000] - loss: 10.506 - mae: 23.662 - mean_q: 35.676 - mean_tau: 0.163\n",
            "\n",
            "Interval 35 (340000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: 0.8164\n",
            "167 episodes - episode_reward: 49.006 [4.000, 60.000] - loss: 10.456 - mae: 23.659 - mean_q: 35.815 - mean_tau: 0.138\n",
            "\n",
            "Interval 36 (350000 steps performed)\n",
            "10000/10000 [==============================] - 99s 10ms/step - reward: 0.8198\n",
            "167 episodes - episode_reward: 49.150 [24.000, 60.000] - loss: 10.892 - mae: 23.888 - mean_q: 36.190 - mean_tau: 0.113\n",
            "\n",
            "Interval 37 (360000 steps performed)\n",
            "10000/10000 [==============================] - 101s 10ms/step - reward: 0.8276\n",
            "166 episodes - episode_reward: 49.639 [6.000, 60.000] - loss: 10.701 - mae: 23.927 - mean_q: 36.129 - mean_tau: 0.088\n",
            "\n",
            "Interval 38 (370000 steps performed)\n",
            "10000/10000 [==============================] - 99s 10ms/step - reward: 0.0284\n",
            "167 episodes - episode_reward: 2.036 [-52.000, 58.000] - loss: 10.274 - mae: 23.176 - mean_q: 34.979 - mean_tau: 0.063\n",
            "\n",
            "Interval 39 (380000 steps performed)\n",
            "10000/10000 [==============================] - 100s 10ms/step - reward: -0.5994\n",
            "167 episodes - episode_reward: -36.012 [-60.000, 24.000] - loss: 9.461 - mae: 21.529 - mean_q: 32.832 - mean_tau: 0.038\n",
            "\n",
            "Interval 40 (390000 steps performed)\n",
            "10000/10000 [==============================] - 99s 10ms/step - reward: -0.6472\n",
            "done, took 3783.829 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3dc14cb4d0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dqn.fit(env, nb_steps=400000, visualize=False, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE1axuCdgcs6",
        "outputId": "82abf996-cb85-47c1-8c8c-ee32849d55ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 150 episodes ...\n",
            "Episode 1: reward: 60.000, steps: 60\n",
            "Episode 2: reward: 60.000, steps: 60\n",
            "Episode 3: reward: 60.000, steps: 60\n",
            "Episode 4: reward: 54.000, steps: 60\n",
            "Episode 5: reward: 60.000, steps: 60\n",
            "Episode 6: reward: 58.000, steps: 60\n",
            "Episode 7: reward: 60.000, steps: 60\n",
            "Episode 8: reward: 56.000, steps: 60\n",
            "Episode 9: reward: 58.000, steps: 60\n",
            "Episode 10: reward: 58.000, steps: 60\n",
            "Episode 11: reward: 60.000, steps: 60\n",
            "Episode 12: reward: 58.000, steps: 60\n",
            "Episode 13: reward: 60.000, steps: 60\n",
            "Episode 14: reward: 60.000, steps: 60\n",
            "Episode 15: reward: 60.000, steps: 60\n",
            "Episode 16: reward: 60.000, steps: 60\n",
            "Episode 17: reward: 60.000, steps: 60\n",
            "Episode 18: reward: 60.000, steps: 60\n",
            "Episode 19: reward: 60.000, steps: 60\n",
            "Episode 20: reward: 60.000, steps: 60\n",
            "Episode 21: reward: 56.000, steps: 60\n",
            "Episode 22: reward: 58.000, steps: 60\n",
            "Episode 23: reward: 60.000, steps: 60\n",
            "Episode 24: reward: 60.000, steps: 60\n",
            "Episode 25: reward: 60.000, steps: 60\n",
            "Episode 26: reward: 60.000, steps: 60\n",
            "Episode 27: reward: 60.000, steps: 60\n",
            "Episode 28: reward: 58.000, steps: 60\n",
            "Episode 29: reward: 54.000, steps: 60\n",
            "Episode 30: reward: 60.000, steps: 60\n",
            "Episode 31: reward: 60.000, steps: 60\n",
            "Episode 32: reward: 60.000, steps: 60\n",
            "Episode 33: reward: 60.000, steps: 60\n",
            "Episode 34: reward: 60.000, steps: 60\n",
            "Episode 35: reward: 60.000, steps: 60\n",
            "Episode 36: reward: 60.000, steps: 60\n",
            "Episode 37: reward: 60.000, steps: 60\n",
            "Episode 38: reward: 58.000, steps: 60\n",
            "Episode 39: reward: 58.000, steps: 60\n",
            "Episode 40: reward: 60.000, steps: 60\n",
            "Episode 41: reward: 60.000, steps: 60\n",
            "Episode 42: reward: 60.000, steps: 60\n",
            "Episode 43: reward: 60.000, steps: 60\n",
            "Episode 44: reward: 58.000, steps: 60\n",
            "Episode 45: reward: 60.000, steps: 60\n",
            "Episode 46: reward: 60.000, steps: 60\n",
            "Episode 47: reward: 60.000, steps: 60\n",
            "Episode 48: reward: 58.000, steps: 60\n",
            "Episode 49: reward: 60.000, steps: 60\n",
            "Episode 50: reward: 60.000, steps: 60\n",
            "Episode 51: reward: 58.000, steps: 60\n",
            "Episode 52: reward: 60.000, steps: 60\n",
            "Episode 53: reward: 60.000, steps: 60\n",
            "Episode 54: reward: 56.000, steps: 60\n",
            "Episode 55: reward: 60.000, steps: 60\n",
            "Episode 56: reward: 60.000, steps: 60\n",
            "Episode 57: reward: 60.000, steps: 60\n",
            "Episode 58: reward: 60.000, steps: 60\n",
            "Episode 59: reward: 54.000, steps: 60\n",
            "Episode 60: reward: 58.000, steps: 60\n",
            "Episode 61: reward: 60.000, steps: 60\n",
            "Episode 62: reward: 50.000, steps: 60\n",
            "Episode 63: reward: 60.000, steps: 60\n",
            "Episode 64: reward: 60.000, steps: 60\n",
            "Episode 65: reward: 54.000, steps: 60\n",
            "Episode 66: reward: 58.000, steps: 60\n",
            "Episode 67: reward: 60.000, steps: 60\n",
            "Episode 68: reward: 58.000, steps: 60\n",
            "Episode 69: reward: 60.000, steps: 60\n",
            "Episode 70: reward: 60.000, steps: 60\n",
            "Episode 71: reward: 60.000, steps: 60\n",
            "Episode 72: reward: 56.000, steps: 60\n",
            "Episode 73: reward: 60.000, steps: 60\n",
            "Episode 74: reward: 58.000, steps: 60\n",
            "Episode 75: reward: 60.000, steps: 60\n",
            "Episode 76: reward: 60.000, steps: 60\n",
            "Episode 77: reward: 60.000, steps: 60\n",
            "Episode 78: reward: 60.000, steps: 60\n",
            "Episode 79: reward: 60.000, steps: 60\n",
            "Episode 80: reward: 60.000, steps: 60\n",
            "Episode 81: reward: 60.000, steps: 60\n",
            "Episode 82: reward: 60.000, steps: 60\n",
            "Episode 83: reward: 60.000, steps: 60\n",
            "Episode 84: reward: 58.000, steps: 60\n",
            "Episode 85: reward: 60.000, steps: 60\n",
            "Episode 86: reward: 60.000, steps: 60\n",
            "Episode 87: reward: 60.000, steps: 60\n",
            "Episode 88: reward: 60.000, steps: 60\n",
            "Episode 89: reward: 60.000, steps: 60\n",
            "Episode 90: reward: 60.000, steps: 60\n",
            "Episode 91: reward: 60.000, steps: 60\n",
            "Episode 92: reward: 60.000, steps: 60\n",
            "Episode 93: reward: 60.000, steps: 60\n",
            "Episode 94: reward: 60.000, steps: 60\n",
            "Episode 95: reward: 54.000, steps: 60\n",
            "Episode 96: reward: 60.000, steps: 60\n",
            "Episode 97: reward: 60.000, steps: 60\n",
            "Episode 98: reward: 60.000, steps: 60\n",
            "Episode 99: reward: 60.000, steps: 60\n",
            "Episode 100: reward: 60.000, steps: 60\n",
            "Episode 101: reward: 60.000, steps: 60\n",
            "Episode 102: reward: 56.000, steps: 60\n",
            "Episode 103: reward: 60.000, steps: 60\n",
            "Episode 104: reward: 60.000, steps: 60\n",
            "Episode 105: reward: 60.000, steps: 60\n",
            "Episode 106: reward: 60.000, steps: 60\n",
            "Episode 107: reward: 60.000, steps: 60\n",
            "Episode 108: reward: 60.000, steps: 60\n",
            "Episode 109: reward: 60.000, steps: 60\n",
            "Episode 110: reward: 60.000, steps: 60\n",
            "Episode 111: reward: 60.000, steps: 60\n",
            "Episode 112: reward: 60.000, steps: 60\n",
            "Episode 113: reward: 54.000, steps: 60\n",
            "Episode 114: reward: 60.000, steps: 60\n",
            "Episode 115: reward: 60.000, steps: 60\n",
            "Episode 116: reward: 60.000, steps: 60\n",
            "Episode 117: reward: 60.000, steps: 60\n",
            "Episode 118: reward: 60.000, steps: 60\n",
            "Episode 119: reward: 60.000, steps: 60\n",
            "Episode 120: reward: 60.000, steps: 60\n",
            "Episode 121: reward: 60.000, steps: 60\n",
            "Episode 122: reward: 60.000, steps: 60\n",
            "Episode 123: reward: 58.000, steps: 60\n",
            "Episode 124: reward: 60.000, steps: 60\n",
            "Episode 125: reward: 58.000, steps: 60\n",
            "Episode 126: reward: 50.000, steps: 60\n",
            "Episode 127: reward: 60.000, steps: 60\n",
            "Episode 128: reward: 60.000, steps: 60\n",
            "Episode 129: reward: 60.000, steps: 60\n",
            "Episode 130: reward: 60.000, steps: 60\n",
            "Episode 131: reward: 56.000, steps: 60\n",
            "Episode 132: reward: 60.000, steps: 60\n",
            "Episode 133: reward: 60.000, steps: 60\n",
            "Episode 134: reward: 58.000, steps: 60\n",
            "Episode 135: reward: 60.000, steps: 60\n",
            "Episode 136: reward: 60.000, steps: 60\n",
            "Episode 137: reward: 60.000, steps: 60\n",
            "Episode 138: reward: 60.000, steps: 60\n",
            "Episode 139: reward: 60.000, steps: 60\n",
            "Episode 140: reward: 60.000, steps: 60\n",
            "Episode 141: reward: 60.000, steps: 60\n",
            "Episode 142: reward: 60.000, steps: 60\n",
            "Episode 143: reward: 60.000, steps: 60\n",
            "Episode 144: reward: 60.000, steps: 60\n",
            "Episode 145: reward: 60.000, steps: 60\n",
            "Episode 146: reward: 60.000, steps: 60\n",
            "Episode 147: reward: 58.000, steps: 60\n",
            "Episode 148: reward: 58.000, steps: 60\n",
            "Episode 149: reward: 60.000, steps: 60\n",
            "Episode 150: reward: 60.000, steps: 60\n",
            "59.18666666666667\n"
          ]
        }
      ],
      "source": [
        "results = dqn.test(env, nb_episodes=150, visualize=False)\n",
        "print(np.mean(results.history['episode_reward']))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
