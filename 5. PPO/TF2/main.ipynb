{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from agent import Agent\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Desktop\\Python\\Reinforcement Learning\\5. PPO\\Continuous\\agent.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  state = T.tensor([observation], dtype=T.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pendulum-v0 Episode 1 total steps 200 avg score -1607.7\n",
      "Pendulum-v0 Episode 2 total steps 400 avg score -1307.6\n",
      "Pendulum-v0 Episode 3 total steps 600 avg score -1316.1\n",
      "Pendulum-v0 Episode 4 total steps 800 avg score -1210.2\n",
      "Pendulum-v0 Episode 5 total steps 1000 avg score -1288.0\n",
      "Pendulum-v0 Episode 6 total steps 1200 avg score -1341.2\n",
      "Pendulum-v0 Episode 7 total steps 1400 avg score -1280.6\n",
      "Pendulum-v0 Episode 8 total steps 1600 avg score -1278.4\n",
      "Pendulum-v0 Episode 9 total steps 1800 avg score -1235.0\n",
      "Pendulum-v0 Episode 10 total steps 2000 avg score -1278.7\n",
      "Pendulum-v0 Episode 11 total steps 2200 avg score -1248.2\n",
      "Pendulum-v0 Episode 12 total steps 2400 avg score -1219.7\n",
      "Pendulum-v0 Episode 13 total steps 2600 avg score -1214.0\n",
      "Pendulum-v0 Episode 14 total steps 2800 avg score -1203.6\n",
      "Pendulum-v0 Episode 15 total steps 3000 avg score -1184.4\n",
      "Pendulum-v0 Episode 16 total steps 3200 avg score -1179.7\n",
      "Pendulum-v0 Episode 17 total steps 3400 avg score -1163.7\n",
      "Pendulum-v0 Episode 18 total steps 3600 avg score -1155.6\n",
      "Pendulum-v0 Episode 19 total steps 3800 avg score -1175.0\n",
      "Pendulum-v0 Episode 20 total steps 4000 avg score -1165.2\n",
      "Pendulum-v0 Episode 21 total steps 4200 avg score -1153.6\n",
      "Pendulum-v0 Episode 22 total steps 4400 avg score -1177.1\n",
      "Pendulum-v0 Episode 23 total steps 4600 avg score -1194.3\n",
      "Pendulum-v0 Episode 24 total steps 4800 avg score -1208.2\n",
      "Pendulum-v0 Episode 25 total steps 5000 avg score -1204.3\n",
      "Pendulum-v0 Episode 26 total steps 5200 avg score -1212.0\n",
      "Pendulum-v0 Episode 27 total steps 5400 avg score -1221.5\n",
      "Pendulum-v0 Episode 28 total steps 5600 avg score -1214.0\n",
      "Pendulum-v0 Episode 29 total steps 5800 avg score -1206.2\n",
      "Pendulum-v0 Episode 30 total steps 6000 avg score -1220.9\n",
      "Pendulum-v0 Episode 31 total steps 6200 avg score -1220.3\n",
      "Pendulum-v0 Episode 32 total steps 6400 avg score -1223.1\n",
      "Pendulum-v0 Episode 33 total steps 6600 avg score -1215.7\n",
      "Pendulum-v0 Episode 34 total steps 6800 avg score -1215.1\n",
      "Pendulum-v0 Episode 35 total steps 7000 avg score -1231.9\n",
      "Pendulum-v0 Episode 36 total steps 7200 avg score -1233.2\n",
      "Pendulum-v0 Episode 37 total steps 7400 avg score -1241.9\n",
      "Pendulum-v0 Episode 38 total steps 7600 avg score -1236.1\n",
      "Pendulum-v0 Episode 39 total steps 7800 avg score -1234.7\n",
      "Pendulum-v0 Episode 40 total steps 8000 avg score -1234.4\n",
      "Pendulum-v0 Episode 41 total steps 8200 avg score -1241.2\n",
      "Pendulum-v0 Episode 42 total steps 8400 avg score -1255.1\n",
      "Pendulum-v0 Episode 43 total steps 8600 avg score -1246.5\n",
      "Pendulum-v0 Episode 44 total steps 8800 avg score -1238.9\n",
      "Pendulum-v0 Episode 45 total steps 9000 avg score -1233.3\n",
      "Pendulum-v0 Episode 46 total steps 9200 avg score -1228.3\n",
      "Pendulum-v0 Episode 47 total steps 9400 avg score -1229.5\n",
      "Pendulum-v0 Episode 48 total steps 9600 avg score -1224.0\n",
      "Pendulum-v0 Episode 49 total steps 9800 avg score -1220.1\n",
      "Pendulum-v0 Episode 50 total steps 10000 avg score -1216.0\n",
      "Pendulum-v0 Episode 51 total steps 10200 avg score -1222.6\n",
      "Pendulum-v0 Episode 52 total steps 10400 avg score -1215.6\n",
      "Pendulum-v0 Episode 53 total steps 10600 avg score -1213.1\n",
      "Pendulum-v0 Episode 54 total steps 10800 avg score -1209.9\n",
      "Pendulum-v0 Episode 55 total steps 11000 avg score -1205.7\n",
      "Pendulum-v0 Episode 56 total steps 11200 avg score -1212.2\n",
      "Pendulum-v0 Episode 57 total steps 11400 avg score -1207.2\n",
      "Pendulum-v0 Episode 58 total steps 11600 avg score -1207.1\n"
     ]
    }
   ],
   "source": [
    "# action adapter idea taken from:\n",
    "# https://github.com/XinJingHao/PPO-Continuous-Pytorch\n",
    "def action_adapter(a, max_a):\n",
    "    return 2 * (a-0.5)*max_a\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_id = 'Pendulum-v0'\n",
    "    random_seed = 0\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    env = gym.make(env_id)\n",
    "    eval_env = gym.make(env_id)\n",
    "    env.seed(random_seed)\n",
    "    eval_env.seed(random_seed)\n",
    "    N = 2048\n",
    "    batch_size = 64\n",
    "    n_epochs = 10\n",
    "    alpha = 0.0003\n",
    "    max_action = env.action_space.high[0]\n",
    "    agent = Agent(n_actions=env.action_space.shape[0], batch_size=batch_size,\n",
    "                  alpha=alpha, n_epochs=n_epochs,\n",
    "                  input_dims=env.observation_space.shape)\n",
    "\n",
    "    score_history = []\n",
    "    max_steps = 1000000\n",
    "    total_steps = 0\n",
    "    traj_length = 0\n",
    "    episode = 1\n",
    "\n",
    "    # for i in range(n_games):\n",
    "    while total_steps < max_steps:\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action, prob = agent.choose_action(observation)\n",
    "            act = action_adapter(action, max_action)\n",
    "            observation_, reward, done, info = env.step(act)\n",
    "            total_steps += 1\n",
    "            traj_length += 1\n",
    "            score += reward\n",
    "            agent.remember(observation, observation_, action, prob, reward, done)\n",
    "            if traj_length % N == 0:\n",
    "                agent.learn()\n",
    "                traj_length = 0\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        print('{} Episode {} total steps {} avg score {:.1f}'.\n",
    "              format(env_id, episode, total_steps, avg_score))\n",
    "        episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def plot_learning_curve(x, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "\n",
    "x = [i+1 for i in range(len(score_history))]\n",
    "plot_learning_curve(x, score_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
