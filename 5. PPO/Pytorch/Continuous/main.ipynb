{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import gym\n",
    "from agent import PPO\n",
    "import os, shutil\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, model, render, steps_per_epoch):\n",
    "    scores = 0\n",
    "    turns = 3\n",
    "    for j in range(turns):\n",
    "        s, done, ep_r, steps = env.reset(), False, 0, 0\n",
    "        while not (done or (steps >= steps_per_epoch)):\n",
    "            # Take deterministic actions at test time\n",
    "            a, logprob_a = model.evaluate(s)\n",
    "            act = lambda a: 2 * (a-0.5)*env.action_space.high[0]\n",
    "            s_prime, r, done, info = env.step(act(a))\n",
    "\n",
    "            ep_r += r\n",
    "            steps += 1\n",
    "            s = s_prime\n",
    "            if render:\n",
    "                env.render()\n",
    "        scores += ep_r\n",
    "    return scores/turns\n",
    "\n",
    "\n",
    "def plot_learning_curve(x, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    \n",
    "    plt.plot(x, running_avg, label=\"Running average\")\n",
    "    plt.plot(x, scores, alpha=0.4)\n",
    "    plt.title('Learning plot')\n",
    "    plt.xlabel(\"Runs\")\n",
    "    plt.ylabel(\"Scores\")\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Environment\n",
    "    random_seed = 0\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "    env.seed(random_seed)\n",
    "    \n",
    "    # Evaluation environment\n",
    "    eval_env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "    eval_env.seed(random_seed)\n",
    "    \n",
    "\n",
    "    # Hyperparameters\n",
    "    kwargs = {\n",
    "        \"input_dims\": env.observation_space.shape, \n",
    "        \"n_actions\": env.action_space.shape[0], \n",
    "        \"gamma\": 0.99, \n",
    "        \"gae_lambda\": 0.95, \n",
    "        \"policy_clip\": 0.2, \n",
    "        \"n_epochs\": 10, \n",
    "        \"net_width\": 256, \n",
    "        \"a_lr\": 3e-4, \n",
    "        \"c_lr\": 3e-4, \n",
    "        \"l2_reg\": 1e-3, \n",
    "        \"dist\": 'GS',\n",
    "        \"batch_size\": 64, \n",
    "        \"entropy_coef\": 1e-3, \n",
    "        \"entropy_coef_decay\": 0.99\n",
    "    }\n",
    "\n",
    "    N = 2048 # lenth of long trajectory\n",
    "    max_steps = env._max_episode_steps\n",
    "    Max_train_steps = 500e3\n",
    "    save_interval = 100e3\n",
    "    eval_interval = 10e3\n",
    "    best_interval = 350e3\n",
    "\n",
    "\n",
    "    if not os.path.exists('model'): \n",
    "        os.mkdir('model')\n",
    "    \n",
    "    if not os.path.exists('best_model'): \n",
    "        os.mkdir('best_model')\n",
    "    \n",
    "    model = PPO(**kwargs)\n",
    "   \n",
    " \n",
    "    traj_lenth = 0\n",
    "    total_steps = 0\n",
    "    score_history = []\n",
    "    \n",
    "    while total_steps < Max_train_steps:\n",
    "        observation, done, steps, ep_r = env.reset(), False, 0, 0\n",
    "\n",
    "        '''Interact & trian'''\n",
    "        while not done:\n",
    "            traj_lenth += 1\n",
    "            steps += 1\n",
    "\n",
    "            action, prob = model.select_action(observation)\n",
    "            \n",
    "            act = lambda a: 2 * (a-0.5)*env.action_space.high[0]  #Action adapter [0,1] to [-max,max]\n",
    "            observation_, reward, done, info = env.step(act(action))\n",
    "            model.remember(observation, observation_, action, prob, reward, done)\n",
    "\n",
    "            observation = observation_\n",
    "            ep_r += reward\n",
    "\n",
    "           \n",
    "            if traj_lenth % N == 0:\n",
    "                model.train()\n",
    "                traj_lenth = 0\n",
    "            \n",
    "            '''record & log'''\n",
    "            if total_steps % eval_interval == 0:\n",
    "                score = evaluate_policy(eval_env, model, False, max_steps)\n",
    "                score_history.append(score)\n",
    "                print('EnvName: Lunar-Lander','steps: {}k'.format(int(total_steps/1000)),'score:', score)\n",
    "\n",
    "            \n",
    "            total_steps += 1\n",
    "\n",
    "            '''save model'''\n",
    "            if total_steps % save_interval==0:\n",
    "                model.save(total_steps)\n",
    "            \n",
    "\n",
    "            ''' best model '''\n",
    "            if total_steps >= best_interval:\n",
    "                if score_history[-1] > score_history[-2]:\n",
    "                    model.best_save()\n",
    "\n",
    "\n",
    "    env.close() \n",
    "    \n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_learning_curve(x, score_history)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "kwargs = {\n",
    "    \"input_dims\": env.observation_space.shape, \n",
    "    \"n_actions\": env.action_space.shape[0], \n",
    "    \"gamma\": 0.99, \n",
    "    \"gae_lambda\": 0.95, \n",
    "    \"policy_clip\": 0.2, \n",
    "    \"n_epochs\": 10, \n",
    "    \"net_width\": 256, \n",
    "    \"a_lr\": 3e-4, \n",
    "    \"c_lr\": 3e-4, \n",
    "    \"l2_reg\": 1e-3, \n",
    "    \"dist\": 'GS',\n",
    "    \"batch_size\": 64, \n",
    "    \"entropy_coef\": 0, \n",
    "    \"entropy_coef_decay\": 0.9998\n",
    "}\n",
    "\n",
    "\n",
    "model = PPO(**kwargs)\n",
    "\n",
    "model.load_best()\n",
    "scores = []\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    obs = env.reset()\n",
    "    actions = []\n",
    "    score = 0\n",
    "    while True:\n",
    "        action, prob = model.evaluate(obs)\n",
    "        act = lambda a: 2 * (a-0.5)*env.action_space.high[0]  #Action adapter [0,1] to [-max,max]\n",
    "        obs, reward, done, info = env.step(act(action))\n",
    "        score += reward\n",
    "        actions.append(action)\n",
    "\n",
    "        if done:\n",
    "            print(f\"Done, points: {score}\")\n",
    "            break\n",
    "    \n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Mean score: {np.mean(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
