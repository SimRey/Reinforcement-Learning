{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYtGONFtgcsy"
      },
      "outputs": [],
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box, Dict\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from agent import Agent\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# This part allows the graph to be drawn\n",
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evjdhHX5gcs0"
      },
      "outputs": [],
      "source": [
        "class Heater(Env):\n",
        "    def __init__(self):\n",
        "\n",
        "        # Action declaration\n",
        "        self.action_space = Dict({\n",
        "            \"discrete\": Discrete(3), # Discrete actions up, down, stay\n",
        "            \"continuous\": Box(low=np.array([0.,]), high=np.array([5.,]), dtype=np.float32) \n",
        "        })\n",
        "\n",
        "        # Temperature array\n",
        "        self.low = np.array([0.])\n",
        "        self.high = np.array([100.])\n",
        "        self.observation_space = Box(low=self.low, high=self.high, dtype=np.float32)\n",
        "        # Set start temp and start time\n",
        "        self.reset()\n",
        "        \n",
        "    def step(self, action):\n",
        "        temp = self.state\n",
        "\n",
        "        d_action = action[\"discrete\"]\n",
        "        c_action = action[\"continuous\"]\n",
        "\n",
        "        if d_action == 0: # Increase temperature\n",
        "            temp += c_action\n",
        "        \n",
        "        elif d_action == 1: # Decrease temperature\n",
        "            temp -= c_action\n",
        "        \n",
        "        elif d_action == 2:\n",
        "            temp += 0\n",
        "        \n",
        "        self.state = np.array(temp, dtype=np.float32)\n",
        "            \n",
        "\n",
        "        # Reduce shower length by 1 second\n",
        "        self.shower_length -= 1 \n",
        "\n",
        "        done = bool(\n",
        "            37. <= self.state <= 39.\n",
        "            or self.shower_length <= 0\n",
        "        )\n",
        "\n",
        "        if done:\n",
        "            reward = 30\n",
        "        else:\n",
        "            reward = -1\n",
        "        \n",
        "            \n",
        "        # Set placeholder for info\n",
        "        info = {}\n",
        "        \n",
        "        # Return step information\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def render(self):\n",
        "        # Implement visualization --> in this case is not built\n",
        "        pass\n",
        "    \n",
        "    def reset(self):\n",
        "        # Reset shower temperature\n",
        "        temp =  random.randint(0, 50)\n",
        "        self.state = np.array([temp], dtype=np.float32)\n",
        "        \n",
        "        # Reset shower time\n",
        "        self.shower_length = 60 \n",
        "        return self.state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al5Psst7gcs1",
        "outputId": "163df462-a328-4386-a653-f9ae5f48e04c"
      },
      "outputs": [],
      "source": [
        "env = Heater()\n",
        "episodes = 10\n",
        "\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    temps = []\n",
        "\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  \n",
        "        temp, reward, done, info = env.step(action)\n",
        "        temps.append(temp) \n",
        "        score +=reward\n",
        "    mean_temp = np.mean(np.array(temps))\n",
        "    print(f'Episode: {episode}, Mean temperature: {mean_temp:.2f} Score: {score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    env = Heater()\n",
        "    N = 2048\n",
        "    batch_size = 64\n",
        "    n_epochs = 10\n",
        "    alpha = 0.0003\n",
        "    max_action = env.action_space[\"continuous\"].high[0]\n",
        "\n",
        "    agent = Agent(actions=env.action_space, input_dims=env.observation_space.shape, \n",
        "        fc1_dims=128, fc2_dims=128, gamma=0.99, alpha=alpha,\n",
        "        gae_lambda=0.95, policy_clip=0.2, batch_size=batch_size, n_epochs=n_epochs)\n",
        "\n",
        "    score_history = []\n",
        "    max_steps = 200e3\n",
        "    total_steps = 0\n",
        "    traj_length = 0\n",
        "    episode = 1\n",
        "\n",
        "    # for i in range(n_games):\n",
        "    while total_steps < max_steps:\n",
        "        observation = env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        while not done:\n",
        "            action_d, probs_d, action_c, probs_c = agent.choose_action(observation)\n",
        "            action = {\n",
        "                \"discrete\": action_d,\n",
        "                \"continuous\": action_c \n",
        "            }\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            total_steps += 1\n",
        "            traj_length += 1\n",
        "            score += reward\n",
        "            agent.remember(observation, observation_, action_d, action_c, probs_d, probs_c, reward, done)\n",
        "            if traj_length % N == 0:\n",
        "                agent.learn()\n",
        "                traj_length = 0\n",
        "            observation = observation_\n",
        "        score_history.append(score)\n",
        "        avg_score = np.mean(score_history[-100:])\n",
        "        print('{} Episode {} total steps {} avg score {:.1f}'.\n",
        "              format(\"Heater\", episode, total_steps, avg_score))\n",
        "        episode += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_learning_curve(x, scores):\n",
        "    running_avg = np.zeros(len(scores))\n",
        "    for i in range(len(running_avg)):\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
        "    plt.plot(x, running_avg)\n",
        "    plt.title('Running average of previous 100 scores')\n",
        "\n",
        "x = [i+1 for i in range(len(score_history))]\n",
        "plot_learning_curve(x, score_history)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "77cc69c02fedbcac23066f25a86a6026b280c8ed56bc60112e8111dd934e143d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
