{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accessible-montana",
   "metadata": {},
   "source": [
    "## <center>Open AI Gym Basics</center>\n",
    "\n",
    "Let's explore the basics of a **gym** environment, later on we wil interact with it and create agents to learn from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effective-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # To slown down the game rendering to human speed\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-surname",
   "metadata": {},
   "source": [
    "### Initializing an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v2\"  # Use the exact same name as stated on gym.openai\n",
    "env = gym.make(env_name)  # use gym.make to create your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-pitch",
   "metadata": {},
   "source": [
    "Thats all! You successfully created the environment!<br />\n",
    "Those steps are identical for all environments.\n",
    "\n",
    "If you want to take a look at your game, gym comes with a handy utils function called **play** for Atari games.<br />\n",
    "Note that you start the game with *space* and move with *a* and *d*\n",
    "\n",
    "How many points can you score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.utils import play  # import the play module\n",
    "play.play(env, zoom=2)  # call the play function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-jackson",
   "metadata": {},
   "source": [
    "Now after getting to know how we can select an environment and interact with it, it is time to dive deeper into the world of gym.\n",
    "\n",
    "Gym offers us many functions to interact with the created environment. The most important are:\n",
    "\n",
    "* reset() - which resets the state of the environment to the initial state. I.e it restarts the game \n",
    "* step(action) - performs an action on your environment. In our case it either starts the game or moves to the left, right, or just remains at the current position. It returns the current state of the environment, the reward, if the game is done and some debugging information\n",
    "* render(mode) - Renders your environment and displays it on your monitor\n",
    "\n",
    "We will get to know those functions in the next few cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-corps",
   "metadata": {},
   "source": [
    "First we will take a look at the **render** function:<br />\n",
    "render(mode) takes one argument, namely the render *mode*. *mode* can take two possible values:\n",
    "* \"human\"\n",
    "* \"rgb_array\"\n",
    "\n",
    "\"human\" directly displays the game on your screen, while \"rgb_array\" returns the array containing the state of the current environment aka the image render shows us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(200):  # Run the environment for 200 steps\n",
    "    env.render(mode=\"human\")  # directly displays you the current state of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()  # as render always opens a new window when called, it is important to close it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = env.render(mode=\"rgb_array\")  # returns the image as a 2d numpy array\n",
    "print(array[60][50])  # display the colour of some pixel (row 60 and column 50 - so the red blocks)\n",
    "\n",
    "plt.imshow(array)  # you can use matplotlib to take a look at your environment - it is the same image as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5323909",
   "metadata": {},
   "outputs": [],
   "source": [
    "array "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-circular",
   "metadata": {},
   "source": [
    "Note that even we ran 1000 steps over the environment, nothing happened! <br />\n",
    "This is because no action was taken on this environment.\n",
    "\n",
    "This is the point where the **step** function joins our game. It performs a selected action on our environment, e.g moving to the left.\n",
    "\n",
    "You can take a look at the possible actions via the **action_space.n** property.\n",
    "It returns 4 for our game which is in line with our above findings that we can either start the game, move left or right, or stand still."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {env.action_space.n} possible actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-mobile",
   "metadata": {},
   "source": [
    "Now it is time to watch some action happening. For this we can select a random possible action.\n",
    "We could do this by using **numpy.random.randint(0, 4)** or by using the provided sample function **env.action_space.sample()** which directly returns you a possible action.\n",
    "\n",
    "Note how the \"ale.lives\" info reduces for each live you lose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(200):\n",
    "    env.render(mode=\"human\")  # display the current state\n",
    "    random_action = env.action_space.sample()  # get the random action\n",
    "    observation, reward, done, info = env.step(random_action) # perform the action the current state of the environment\n",
    "    print(f\"Reward: {reward}, Done: {done}, Info: {info}\")\n",
    "    time.sleep(0.01)  # slow down the game a bit\n",
    "env.close()  # dont forget to close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-amount",
   "metadata": {},
   "source": [
    "If we would now again render the environment, we continue at the same stage we left - thus to start again we need to call the **reset** function.\n",
    "The reset function returns the initial state of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()  # Old state of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-approval",
   "metadata": {},
   "source": [
    "Now we can again take a look at the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-estate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    env.render(mode=\"human\")  # display the current state\n",
    "    random_action = env.action_space.sample()  # get the random action\n",
    "    observation, reward, done, info = env.step(random_action) # perform the action the current state of the environment\n",
    "    print(f\"Reward: {reward}, Done: {done}, Info: {info}\")\n",
    "    if done:  # with the done variable we can leave the loop if gym notifies us that the game is over\n",
    "        break\n",
    "    time.sleep(0.01)  # slow down the game a bit\n",
    "env.close()  # dont forget to close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-carnival",
   "metadata": {},
   "source": [
    "As you might have noticed, there are always two versions of an environment:\n",
    "\n",
    "1. RAM Version\n",
    "2. Normal Version / Image based version\n",
    "\n",
    "Those version differ in the observation which gym returns to you. <br />\n",
    "The Ram Version only returns some properties (like the coordinates of the ball), while the normal version returns images (i.e exact the image that render() shows us).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name_ram = \"Breakout-ram-v0\"  # use the RAM version\n",
    "env_ram = gym.make(env_name_ram)  # create the RAM version of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_ram.render(mode=\"human*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_ram.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_ram.reset().shape # you can use the reset function to inspect the initial state of your environment.\n",
    "                      # There are only 128 values instead of the whole image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset().shape  # Our original environment returns an image as observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-classification",
   "metadata": {},
   "source": [
    "The next notebooks shows an application, where we can directly use the values the RAM version returns to us to create our first little agent that plays a game for us.\n",
    "\n",
    "Feel free to change the environments in this chapter and try all the available games.\n",
    "What highscores are you able to achieve?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
