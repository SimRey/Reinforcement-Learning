{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overhead-satisfaction",
   "metadata": {},
   "source": [
    "## <center>Keras-RL DQN Model</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-firewall",
   "metadata": {},
   "source": [
    "In this notebook we will create our first Reinforcement Learning agent via keras-RL2 taking the *Cartpole* as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chronic-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # to reduce the game speed when playing manually\n",
    "\n",
    "import gym  # Contains the game we want to play\n",
    "\n",
    "# import necessary blocks from keras to build the Deep Learning backbone of our agent\n",
    "from tensorflow.keras.models import Sequential  \n",
    "from tensorflow.keras.layers import Dense, Activation, InputLayer\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import Adam  # Adam optimizer\n",
    "\n",
    "# Now the keras-rl2 agent. Dont get confused as it is only called rl and not keras-rl\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-warehouse",
   "metadata": {},
   "source": [
    "### a. Environment set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e3d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    return env\n",
    "\n",
    "env = recall()\n",
    "env.reset()\n",
    "\n",
    "for _ in range(9):\n",
    "    env.render(mode=\"human\")  \n",
    "    random_action = env.action_space.sample()\n",
    "    env.step(random_action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7d50d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 possible actions and 4 observations\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape[0]\n",
    "print(f\"There are {num_actions} possible actions and {num_observations} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70570a4",
   "metadata": {},
   "source": [
    "### b. DQN agent set up\n",
    "The DQN agent created with keras-RL2, needs the following parameters to be created:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2cf70e",
   "metadata": {},
   "source": [
    "**1. Model**\n",
    "\n",
    "The model is the ANN, in this case we will use the same as the one implemented in the Manual DQN notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5fe9d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                80        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(1, num_observations)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d1816",
   "metadata": {},
   "source": [
    "**2. nb_actions**\n",
    "\n",
    "Number of actions --> already defined\n",
    "\n",
    "**3. memory**\n",
    "\n",
    "The action replay memory. You can choose between the *SequentialMemory()* and *EpisodeParameterMemory()* which is only used for one RL agent called *CEM*. Sequential Memory is for storing observations (optimized circular buffer)\n",
    "\n",
    "Here we initialize the circular buffer with a limit of 20000 and a window length of 1. The window length describes the number of subsequent actions stored for a state. This will be demonstrated in the next lecture, when we start dealing with images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "309410eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory  \n",
    "memory = SequentialMemory(limit=20000, window_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba750fd7",
   "metadata": {},
   "source": [
    "**4. nb_steps_warmup**\n",
    "\n",
    "How many iterations without training - Used to fill the memory\n",
    "\n",
    "**5. target_model_update**\n",
    "\n",
    "When do we update the target model?\n",
    "\n",
    "**6. Action Selection policy**\n",
    "\n",
    "There are many policies to chose from, some of them like the *LinearAnnealedPolicy()*, are referred as outter policies and take an inner policy such as *SoftmaxPolicy()*, *EpsGreedyQPolicy()*, *GreedyQPolicy()*, *GreedyQPolicy()*, *MaxBoltzmannQPolicy()* and *BoltzmannGumbelQPolicy()*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "681a8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), # inner policy\n",
    "                              attr='eps', # attribute \n",
    "                              value_max=1.0, # max value of the attribute\n",
    "                              value_min=0.001, # min value of the attribute \n",
    "                              value_test=0.0005, # small value to test the model --> explotation\n",
    "                              nb_steps=200000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-audience",
   "metadata": {},
   "source": [
    "Now we create the DQN Agent based on the defined model (**model**), the possible actions (**nb_actions**) (left and right in this case), the circular buffer (**memory**), the burnin or warmup phase (**10**), how often the target model gets updated (**100**) and the policy (**policy**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "piano-exercise",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "c:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dd1c295310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=num_actions, \n",
    "               memory=memory, \n",
    "               nb_steps_warmup=10,\n",
    "               target_model_update=100, \n",
    "               policy=policy)\n",
    "\n",
    "# Compilation\n",
    "dqn.compile(Adam(learning_rate=0.0001), metrics=['mae']) \n",
    "\n",
    "# Now we run the training for 20000 steps. You can change visualize=True if you want to watch your model learning. \n",
    "# Keep in mind that this increases the running time\n",
    "\n",
    "dqn.fit(env, nb_steps=200000, visualize=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "surprised-symphony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 232.000, steps: 232\n",
      "Episode 2: reward: 500.000, steps: 500\n",
      "Episode 3: reward: 283.000, steps: 283\n",
      "Episode 4: reward: 334.000, steps: 334\n",
      "Episode 5: reward: 232.000, steps: 232\n"
     ]
    }
   ],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
