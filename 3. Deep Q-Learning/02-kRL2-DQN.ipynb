{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overhead-satisfaction",
   "metadata": {},
   "source": [
    "## <center>Keras-RL DQN Model</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-firewall",
   "metadata": {},
   "source": [
    "In this notebook we will create our first Reinforcement Learning agent via keras-RL2 taking the *Cartpole* as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chronic-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # to reduce the game speed when playing manually\n",
    "\n",
    "import gym  # Contains the game we want to play\n",
    "\n",
    "# import necessary blocks from keras to build the Deep Learning backbone of our agent\n",
    "from tensorflow.keras.models import Sequential  \n",
    "from tensorflow.keras.layers import Dense, Activation, InputLayer\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import Adam  # Adam optimizer\n",
    "\n",
    "# Now the keras-rl2 agent. Dont get confused as it is only called rl and not keras-rl\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-warehouse",
   "metadata": {},
   "source": [
    "### a. Environment set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e3d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    return env\n",
    "\n",
    "env = recall()\n",
    "env.reset()\n",
    "\n",
    "for _ in range(9):\n",
    "    env.render(mode=\"human\")  \n",
    "    random_action = env.action_space.sample()\n",
    "    env.step(random_action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7d50d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 possible actions and 4 observations\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape[0]\n",
    "print(f\"There are {num_actions} possible actions and {num_observations} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70570a4",
   "metadata": {},
   "source": [
    "### b. DQN agent set up\n",
    "The DQN agent created with keras-RL2, needs the following parameters to be created:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2cf70e",
   "metadata": {},
   "source": [
    "**1. Model**\n",
    "\n",
    "The model is the ANN, in this case we will use the same as the one implemented in the Manual DQN notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5fe9d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                160       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,338\n",
      "Trainable params: 2,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(1, num_observations)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d1816",
   "metadata": {},
   "source": [
    "**2. nb_actions**\n",
    "\n",
    "Number of actions --> already defined\n",
    "\n",
    "**3. memory**\n",
    "\n",
    "The action replay memory. You can choose between the *SequentialMemory()* and *EpisodeParameterMemory()* which is only used for one RL agent called *CEM*. Sequential Memory is for storing observations (optimized circular buffer)\n",
    "\n",
    "Here we initialize the circular buffer with a limit of 20000 and a window length of 1. The window length describes the number of subsequent actions stored for a state. This will be demonstrated in the next lecture, when we start dealing with images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "309410eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory  \n",
    "memory = SequentialMemory(limit=20000, window_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba750fd7",
   "metadata": {},
   "source": [
    "**4. nb_steps_warmup**\n",
    "\n",
    "How many iterations without training - Used to fill the memory\n",
    "\n",
    "**5. target_model_update**\n",
    "\n",
    "When do we update the target model?\n",
    "\n",
    "**6. Action Selection policy**\n",
    "\n",
    "There are many policies to chose from, some of them like the *LinearAnnealedPolicy()*, are referred as outter policies and take an inner policy such as *SoftmaxPolicy()*, *EpsGreedyQPolicy()*, *GreedyQPolicy()*, *GreedyQPolicy()*, *MaxBoltzmannQPolicy()* and *BoltzmannGumbelQPolicy()*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "681a8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), # inner policy\n",
    "                              attr='eps', # attribute \n",
    "                              value_max=1.0, # max value of the attribute\n",
    "                              value_min=0.001, # min value of the attribute \n",
    "                              value_test=0.0005, # small value to test the model --> explotation\n",
    "                              nb_steps=200000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-audience",
   "metadata": {},
   "source": [
    "Now we create the DQN Agent based on the defined model (**model**), the possible actions (**nb_actions**) (left and right in this case), the circular buffer (**memory**), the burnin or warmup phase (**10**), how often the target model gets updated (**100**) and the policy (**policy**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "piano-exercise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 200000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 8:12 - reward: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "c:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "421 episodes - episode_reward: 23.717 [8.000, 121.000] - loss: 8.657 - mae: 13.803 - mean_q: 26.795 - mean_eps: 0.975\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "368 episodes - episode_reward: 27.196 [8.000, 102.000] - loss: 35.264 - mae: 42.799 - mean_q: 87.094 - mean_eps: 0.925\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: 1.0000\n",
      "328 episodes - episode_reward: 30.485 [8.000, 115.000] - loss: 139.249 - mae: 109.835 - mean_q: 226.613 - mean_eps: 0.875\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: 1.0000\n",
      "284 episodes - episode_reward: 35.070 [9.000, 131.000] - loss: 506.604 - mae: 229.148 - mean_q: 473.489 - mean_eps: 0.825\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "238 episodes - episode_reward: 42.139 [8.000, 188.000] - loss: 1307.097 - mae: 398.757 - mean_q: 822.530 - mean_eps: 0.775\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: 1.0000\n",
      "193 episodes - episode_reward: 51.653 [11.000, 216.000] - loss: 3026.643 - mae: 610.585 - mean_q: 1256.975 - mean_eps: 0.725\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 86s 9ms/step - reward: 1.0000\n",
      "161 episodes - episode_reward: 62.410 [12.000, 250.000] - loss: 5187.269 - mae: 821.108 - mean_q: 1687.822 - mean_eps: 0.675\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: 1.0000\n",
      "142 episodes - episode_reward: 69.507 [10.000, 290.000] - loss: 6814.345 - mae: 993.823 - mean_q: 2039.569 - mean_eps: 0.625\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "111 episodes - episode_reward: 90.369 [8.000, 257.000] - loss: 6225.883 - mae: 1061.581 - mean_q: 2176.131 - mean_eps: 0.575\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "69 episodes - episode_reward: 144.942 [14.000, 500.000] - loss: 4590.565 - mae: 1042.696 - mean_q: 2132.408 - mean_eps: 0.525\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "49 episodes - episode_reward: 204.959 [31.000, 424.000] - loss: 3348.037 - mae: 937.650 - mean_q: 1912.134 - mean_eps: 0.476\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "45 episodes - episode_reward: 221.511 [18.000, 440.000] - loss: 1578.157 - mae: 742.911 - mean_q: 1511.961 - mean_eps: 0.426\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "44 episodes - episode_reward: 225.727 [25.000, 360.000] - loss: 636.861 - mae: 518.500 - mean_q: 1052.765 - mean_eps: 0.376\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: 1.0000\n",
      "37 episodes - episode_reward: 270.730 [38.000, 500.000] - loss: 265.219 - mae: 331.215 - mean_q: 672.493 - mean_eps: 0.326\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "39 episodes - episode_reward: 252.282 [30.000, 472.000] - loss: 106.707 - mae: 193.017 - mean_q: 391.918 - mean_eps: 0.276\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "60 episodes - episode_reward: 169.850 [23.000, 417.000] - loss: 51.096 - mae: 120.695 - mean_q: 244.276 - mean_eps: 0.226\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "42 episodes - episode_reward: 235.786 [18.000, 429.000] - loss: 38.452 - mae: 82.724 - mean_q: 166.486 - mean_eps: 0.176\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "43 episodes - episode_reward: 234.209 [22.000, 438.000] - loss: 20.851 - mae: 70.866 - mean_q: 142.615 - mean_eps: 0.126\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 83s 8ms/step - reward: 1.0000\n",
      "49 episodes - episode_reward: 203.755 [168.000, 269.000] - loss: 3.285 - mae: 50.788 - mean_q: 102.040 - mean_eps: 0.076\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 83s 8ms/step - reward: 1.0000\n",
      "done, took 1686.863 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b6ea0a7f10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=num_actions, \n",
    "               memory=memory, \n",
    "               nb_steps_warmup=10,\n",
    "               target_model_update=100, \n",
    "               policy=policy)\n",
    "\n",
    "# Compilation\n",
    "dqn.compile(Adam(learning_rate=0.0001), metrics=['mae']) \n",
    "\n",
    "# Now we run the training for 20000 steps. You can change visualize=True if you want to watch your model learning. \n",
    "# Keep in mind that this increases the running time\n",
    "\n",
    "dqn.fit(env, nb_steps=200000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "surprised-symphony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 220.000, steps: 220\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 208.000, steps: 208\n",
      "Episode 4: reward: 239.000, steps: 239\n",
      "Episode 5: reward: 283.000, steps: 283\n"
     ]
    }
   ],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
