{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stopped-alberta",
   "metadata": {},
   "source": [
    "## <center>Manually Creating a DQN Model </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-porcelain",
   "metadata": {},
   "source": [
    "### Deep-Q-Learning\n",
    "In this notebook we will create our first Deep Reeinforcement Learning model, called Deep-Q-Network (DQN).\n",
    "We are again using a simple environment from openai gym. However, you will soon see the enormous gain we will get by switching from standard Q-Learning to Deep Q Learning.\n",
    "\n",
    "In this notebook we again take a look at the CartPole problem (https://gym.openai.com/envs/CartPole-v1/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hybrid-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, InputLayer  \n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fifty-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(mode=None):\n",
    "    env = gym.make('CartPole-v1', render_mode=mode)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-exploration",
   "metadata": {},
   "source": [
    "Remember, the goal of the CartPole challenge was to balance the stick upright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "southeast-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = recall(\"human\")\n",
    "env.reset()\n",
    "\n",
    "for _ in range(9):\n",
    "    env.render()  \n",
    "    random_action = env.action_space.sample()\n",
    "    env.step(random_action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf18acd",
   "metadata": {},
   "source": [
    "### The Artificial Neural Network\n",
    "To build our network, we first need to find out how many actions and observation our environment has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "widespread-oxide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 possible actions and 4 observations\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape[0]  # You can use this command to get the number of observations\n",
    "print(f\"There are {num_actions} possible actions and {num_observations} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-alaska",
   "metadata": {},
   "source": [
    "So our network needs to have an input dimension of 4 and an output dimension of 2.\n",
    "In between we are free to chose.\n",
    "\n",
    "Let's just say we want to use a four layer architecture:\n",
    "\n",
    "\n",
    "1. The first layer has 16 neurons\n",
    "2. The second layer has 32 neurons\n",
    "4. The fourth layer (output layer) has 2 neurons\n",
    "\n",
    "This yields 690 parameters\n",
    "$$ \\text{4 observations} * 16 (\\text{neurons}) + 16 (\\text{bias}) + (16*32) + 32 + (32*2)+2 = 690$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "insured-words",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                80        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(InputLayer(input_shape=(num_observations, )))\n",
    "\n",
    "model.add(Dense(16,))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation('linear')) # used to chose one of the neurons\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-sitting",
   "metadata": {},
   "source": [
    "Now we have our model which takes an observation as input and outputs a value for each action.\n",
    "The higher the value, the more likely that this value is a suitable action for the current observation\n",
    "\n",
    "As stated in the lecture, Deep-Q-Learning works better when using a target network.\n",
    "So let's just copy the above network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advanced-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = clone_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7991ba",
   "metadata": {},
   "source": [
    "### Hyperparameters and Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interim-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 250\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "epsilon = 1.0\n",
    "EPSILON_REDUCE = 0.995  # is multiplied with epsilon each epoch to reduce it\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-simple",
   "metadata": {},
   "source": [
    "Let us use the epsilon greedy action selection function once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coordinate-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_selection(model, epsilon, observation):\n",
    "    random_number = np.random.random()\n",
    "    if random_number > epsilon:\n",
    "        prediction = model.predict(observation)  # perform the prediction on the observation\n",
    "        action = np.argmax(prediction)  # Chose the action with the higher value\n",
    "    else:\n",
    "        action = np.random.randint(0, env.action_space.n)  # Else use random action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a10c4",
   "metadata": {},
   "source": [
    "----\n",
    "#### **Deque**\n",
    "\n",
    "A replay buffer is needed. We can use the **deque** data structure for this, which already implements the circular behavior. The *maxlen* argument specifies the number of elements the buffer can store between he overwrites them at the beginning. \n",
    "\n",
    "The following example shows how a **deque** works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brilliant-receptor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([0], maxlen=5)\n",
      "deque([0, 1], maxlen=5)\n",
      "deque([0, 1, 2], maxlen=5)\n",
      "deque([0, 1, 2, 3], maxlen=5)\n",
      "deque([0, 1, 2, 3, 4], maxlen=5)\n",
      "deque([1, 2, 3, 4, 5], maxlen=5)\n",
      "deque([2, 3, 4, 5, 6], maxlen=5)\n",
      "deque([3, 4, 5, 6, 7], maxlen=5)\n",
      "deque([4, 5, 6, 7, 8], maxlen=5)\n",
      "deque([5, 6, 7, 8, 9], maxlen=5)\n",
      "deque([6, 7, 8, 9, 10], maxlen=5)\n",
      "deque([6, 7, 8, 9, 10], maxlen=5)\n"
     ]
    }
   ],
   "source": [
    "deque_1 = deque(maxlen=5)\n",
    "for i in range(11):\n",
    "    deque_1.append(i)\n",
    "    print(deque_1)\n",
    "print(deque_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9880fbd",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-august",
   "metadata": {},
   "source": [
    "Let's say we allow our replay buffer a maximum size of 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "thrown-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=20000)\n",
    "update_target_model = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c58c8",
   "metadata": {},
   "source": [
    "----\n",
    "#### **Unzipping and pairing**\n",
    "\n",
    "As mentioned in the lecture, action replaying is crucial for Deep Q-Learning. The following cell implements one version of the action replay algorithm. It uses the zip statement paired with the * (Unpacking Argument Lists) operator to create batches from the samples for efficient prediction and training. The zip statement returns all corresponding pairs from each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "constant-timothy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 7) (2, 5, 8) (3, 6, 9)\n"
     ]
    }
   ],
   "source": [
    "test_tuple = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n",
    "zipped_list = list(zip(*test_tuple))\n",
    "a, b, c = zipped_list\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936e91d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-vegetation",
   "metadata": {},
   "source": [
    "Now it's time to write the replay function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "scheduled-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(replay_buffer, batch_size, model, target_model):\n",
    "    \n",
    "    # As long as the buffer has not enough elements we do nothing\n",
    "    if len(replay_buffer) < batch_size: \n",
    "        return\n",
    "    \n",
    "    # Take a random sample from the buffer with size batch_size\n",
    "    samples = random.sample(replay_buffer, batch_size)  \n",
    "    \n",
    "    # to store the targets predicted by the target network for training\n",
    "    target_batch = []  \n",
    "    \n",
    "    # Efficient way to handle the sample by using the zip functionality\n",
    "    zipped_samples = list(zip(*samples))  \n",
    "    states, actions, rewards, new_states, dones = zipped_samples  \n",
    "    \n",
    "    # Predict targets for all states from the sample\n",
    "    targets = target_model.predict(np.array(states))\n",
    "    \n",
    "    # Predict Q-Values for all new states from the sample\n",
    "    q_values = model.predict(np.array(new_states))  \n",
    "    \n",
    "    # Now we loop over all predicted values to compute the actual targets\n",
    "    for i in range(batch_size):  \n",
    "        \n",
    "        # Take the maximum Q-Value for each sample\n",
    "        q_value = max(q_values[i][0])  \n",
    "        \n",
    "        # Store the ith target in order to update it according to the formula\n",
    "        target = targets[i].copy()  \n",
    "        if dones[i]:\n",
    "            target[0][actions[i]] = rewards[i]\n",
    "        else:\n",
    "            target[0][actions[i]] = rewards[i] + q_value * GAMMA\n",
    "        target_batch.append(target)\n",
    "\n",
    "    # Fit the model based on the states and the updated targets for 1 epoch\n",
    "    model.fit(np.array(states), np.array(target_batch), epochs=1, verbose=0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-testing",
   "metadata": {},
   "source": [
    "We need to update our target network every once in a while. <br />\n",
    "Keras provides the *set_weights()* and *get_weights()* methods which do the work for us, so we only need to check whether we hit an update epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stainless-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_handler(epoch, update_target_model, model, target_model):\n",
    "    if epoch > 0 and epoch % update_target_model == 0:\n",
    "        target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4755d07",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-michael",
   "metadata": {},
   "source": [
    "Now it is time to write the training loop! <br />\n",
    "First we compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sharing-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-interface",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_so_far = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    env = recall()\n",
    "    observation = env.reset()\n",
    "    observation = observation[0]  # Get inital state\n",
    "    \n",
    "    # Keras expects the input to be of shape [1, X] thus we have to reshape\n",
    "    #observation = observation.reshape([1, 4])  \n",
    "    done = False  \n",
    "    \n",
    "    points = 0\n",
    "    while not done:  # as long current run is active\n",
    "        \n",
    "        # Select action acc. to strategy\n",
    "        action = action_selection(model, epsilon, observation)\n",
    "        \n",
    "        # Perform action and get next state\n",
    "        next_observation, reward, done, *info = env.step(action)  \n",
    "        #next_observation = next_observation.reshape([1, 4])  # Reshape!!\n",
    "        replay_buffer.append((observation, action, reward, next_observation, done))  # Update the replay buffer\n",
    "        observation = next_observation  # update the observation\n",
    "        points+=1\n",
    "\n",
    "        # Most important step! Training the model by replaying\n",
    "        replay(replay_buffer, 2, model, target_model)\n",
    "\n",
    "    \n",
    "    epsilon *= EPSILON_REDUCE  # Reduce epsilon\n",
    "    \n",
    "    # Check if we need to update the target model\n",
    "    update_model_handler(epoch, update_target_model, model, target_model)\n",
    "    \n",
    "    if points > best_so_far:\n",
    "        best_so_far = points\n",
    "\n",
    "    if epoch == 0 or epoch %10 == 0:\n",
    "        print(f\"{epoch}: Points reached: {points} - epsilon: {epsilon} - Best: {best_so_far}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e969bb",
   "metadata": {},
   "source": [
    "### Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cc628",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = recall(\"human\")\n",
    "observation = env.reset()\n",
    "observation = observation[0]\n",
    "for counter in range(300):\n",
    "    env.render()\n",
    "    \n",
    "    # Get discretized observation\n",
    "    action = np.argmax(model.predict(observation))\n",
    "    \n",
    "    # Perform the action \n",
    "    observation, reward, done, *info = env.step(action) # Finally perform the action\n",
    "    \n",
    "    if done:\n",
    "        print(f\"done\")\n",
    "        break\n",
    "    time.sleep(0.1)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
