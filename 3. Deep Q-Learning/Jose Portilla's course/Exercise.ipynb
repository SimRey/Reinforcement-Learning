{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attended-novel",
   "metadata": {},
   "source": [
    "# Keras-RL DQN Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-agency",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement your first keras-rl agent based on the **Acrobot** environment (https://gym.openai.com/envs/Acrobot-v1/) <br />\n",
    "The goal of this environment is to maneuver the robot arm upwards above the line with as little steps as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "knowing-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential  \n",
    "from tensorflow.keras.layers import Dense, Activation, InputLayer\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import Adam  \n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory  \n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-tenant",
   "metadata": {},
   "source": [
    "**TASK: Create the environment** <br />\n",
    "The name is: *Acrobot-v1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compound-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall():\n",
    "    env = gym.make('Acrobot-v1')\n",
    "    return env\n",
    "\n",
    "env = recall()\n",
    "env.reset()\n",
    "\n",
    "for _ in range(300):\n",
    "    env.render(mode=\"human\")  \n",
    "    random_action = env.action_space.sample()\n",
    "    env.step(random_action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "israeli-assumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: 3\n",
      "Observation Space: 6\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape[0]\n",
    "print(f\"Action Space: {env.action_space.n}\")\n",
    "print(f\"Observation Space: {num_observations}\")\n",
    "\n",
    "assert num_actions == 3 and num_observations == 6 , \"Wrong environment!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-tuesday",
   "metadata": {},
   "source": [
    "**TASK: Create the Neural Network for your Deep-Q-Agent** <br />\n",
    "Take a look at the size of the action space and the size of the observation space.\n",
    "You are free to chose any architecture you want! <br />\n",
    "Hint: It already works with three layers, each having 64 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mexican-deputy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 6)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                112       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,747\n",
      "Trainable params: 13,747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(1, num_observations)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-mixture",
   "metadata": {},
   "source": [
    "**TASK: Initialize the circular buffer**<br />\n",
    "Make sure you set the limit appropriately (50000 works well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "short-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-grain",
   "metadata": {},
   "source": [
    "**TASK: Use the epsilon greedy action selection strategy with *decaying* epsilon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "polished-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), # inner policy\n",
    "                              attr='eps', # attribute \n",
    "                              value_max=1.0, # max value of the attribute\n",
    "                              value_min=0.1, # min value of the attribute \n",
    "                              value_test=0.05, # small value to test the model --> explotation\n",
    "                              nb_steps=50000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-straight",
   "metadata": {},
   "source": [
    "**TASK: Create the DQNAgent** <br />\n",
    "Feel free to play with the nb_steps_warump, target_model_update, batch_size and gamma parameters. <br />\n",
    "Hint:<br />\n",
    "You can try *nb_steps_warmup*=1000, *target_model_update*=1000, *batch_size*=32 and *gamma*=0.99 as a first guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "terminal-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=num_actions, \n",
    "               memory=memory, \n",
    "               nb_steps_warmup=1000,\n",
    "               target_model_update=1000, \n",
    "               policy=policy,\n",
    "               gamma=0.99,\n",
    "               batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-shooting",
   "metadata": {},
   "source": [
    "**TASK: Compile the model** <br />\n",
    "Feel free to explore the effects of different optimizers and learning rates.\n",
    "You can try Adam with a learning rate of 1e-3 as a first guess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "damaged-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=0.001), metrics=['mae']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-belgium",
   "metadata": {},
   "source": [
    "**TASK: Fit the model** <br />\n",
    "150,000 steps should be a very good starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adverse-determination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1652ad499a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=150000, visualize=False, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-nightmare",
   "metadata": {},
   "source": [
    "**TASK: Evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "framed-hawaii",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -94.000, steps: 95\n",
      "Episode 2: reward: -83.000, steps: 84\n",
      "Episode 3: reward: -95.000, steps: 96\n",
      "Episode 4: reward: -69.000, steps: 70\n",
      "Episode 5: reward: -84.000, steps: 85\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
